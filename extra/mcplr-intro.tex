\documentclass[doc]{apa}
\usepackage[utopia]{mathdesign}
\usepackage{graphicx,apacite,amsmath,rotating,verbatim,epsfig,subfigure}
\usepackage{dcolumn}

\title{mcplR: An R package for multiple cue probability learning models}
\author{Maarten Speekenbrink}
\affiliation{Department of Psychology \\ University College London}

\abstract{

}

\ifapamodedoc{%
\leftheader{Speekenbrink}
\rightheader{MCPLR}
\acknowledgements{This research was supported by the ESRC Centre for Economic Learning and Social Evolution (ELSE).

Address correspondence to M. Speekenbrink, Department of Psychology, University College London, Gower Street, London WC1E 6BT, England, e-mail: \texttt{m.speekenbrink@ucl.ac.uk}}
}

\ifapamodejou{%
\leftheader{Speekenbrink}
\rightheader{MCPLR}
\acknowledgements{Address correspondence to M. Speekenbrink, Department of Psychology, University College London, Gower Street, London WC1E 6BT, England, e-mail: \texttt{m.speekenbrink@ucl.ac.uk}}
}

\ifapamodeman{%
	\note{
	\vspace{6em}
	\begin{flushleft}
    Dr. M. Speekenbrink\\
    Department of Psychology\\
    University College London \\
    Gower Street \\
    London WC1E 6BT \\ 
    England\\
    Tel:  +44 (0) 20 7679 7572 \\
    Fax: +44 (0) 20 7436 4276 \\
    E-mail: m.speekenbrink@ucl.ac.uk\\
   \end{flushleft}}}
{% else, i.e., in jou and doc mode 
\note{Draft of \today}}

\journal{To be submitted}

\renewcommand{\vec}[1]{\text{\bf{#1}}}
\newcommand{\mat}[1]{\text{\bf{#1}}}
\newcommand{\tr}{\text{tr}}
\newcommand{\logit}[1]{\log \left( \frac{#1}{1 - #1} \right)}
\newcommand{\logodds}[2]{\log \left( \frac{#1}{#2} \right)}
\newcommand{\slogit}[1]{\log( \tfrac{#1}{1 - #1})}
\newcommand{\mean}[1]{\overline{#1}}
\newcommand{\sign}{\text{sgn}}
\newcommand{\tif}{\text{ if }}

\newcommand{\greekv}[1]{\mbox{\boldmath$#1$}}
\newcommand{\greekm}[1]{\mbox{\boldmath$#1$}}
\newcommand{\thetab}{\mbox{\boldmath$\theta$}}
\newcommand{\betab}{\mbox{\boldmath$\beta$}}
\newcommand{\mub}{\mbox{\boldmath$\mu$}}
\newcommand{\Sigmab}{\mbox{\boldmath$\Sigma$}}
\newcommand{\sigmab}{\mbox{\boldmath$\sigma$}}
\newcommand{\MSE}{\text{MS}_e}
\newcommand{\diag}{\text{diag}}
\newcommand{\mtr}{^{\textsf{T}}}

\newcommand{\code}[1]{{\ttfamily{#1}}}

\newcolumntype{d}[1]{D{.}{.}{#1}}

\begin{document}
\maketitle

Multiple cue probability learning (MCPL) tasks involve predicting a criterion variable on the basis of a number of cues which are probabilistically related to the criterion. When the criterion is a categorical variable, these tasks are more commonly referred to as category learning; when the criterion is a metric variable, they are also referred to as social judgement tasks. %  These tasks are quite general type of task is quite general. A numerical criterion variable is often used in Social Judgement Theory. When the criterion is a nominal variable, it is also common to speak of category learning tasks. 

A large number of formal models have been proposed to describe how people learn to perform MCPL tasks. The \code{mcplR} package for the R statistical software environment allows users to estimate a number of these. Moreover, its flexible design makes it relatively easy to implement other MCPL models. %It has been designed The \code{mcplR} package has been designed such that users can relatively easily add other models. 

After introducing the general structure of MCPL models, the main features of the package are illustrated through the package's implementation of the Rescorla-Wagner model. %We will describe the features of the package through the package's implementation of the Generalized Context Model (GCM). 
We will also briefly illustrate how to specify a new model. 

\section{The general structure of MCPL models}

In MCPL tasks, the objective is to predict a criterion variable $Y_t$ on the basis of a number of cues $X_{jt}$. On each trial $t$, cue values are presented, and a prediction $R_t$ is made about the value of the criterion.

An MCPL model consists of a learning and a response model. The learning model describes how a representation of the environment is updated as new data comes in. The response model describes how this representation is used to form predictions of the environment. A graphical representation is given in Figure~\ref{fig:mcplmodel}. A main feature of MCPL models is that responses $R_t$ are conditionally independent given representations $\psi_t$, i.e.
\begin{equation}
P(R_1,\ldots,R_T|\psi_1,\ldots,\psi_T,\greekv{\theta}_R) = \prod_{t=1}^T P(R_t|\psi_t,\greekv{\theta}_R)
\end{equation}
While the response model is always a statistical model, in many cases the learning model is not. Models such as the GCM are deterministic, in the sense that a particular set of observations $x,y$ and parameters always result in the same  (e.g., there is a many-to-one mapping from $(\vec{x},\vec{y},\greekv{\theta}) \rightarrow \psi_t$).
 
\section{The mcplR package}

In the implementation of MCPL models, we distinguish between learning and response models. A learning model

At the heart of the mcplR package is the class \code{McplModel}. An \code{McplModel} contains of two submodels, namely a \code{LearningModel} and a \code{ResponseModel}. Both these are derived from a general \code{McplBaseModel}, which contains a slot for a dependent variable ($y$), a slot for predictor variables ($x$), and a slot for a list with parameters. For a \code{LearningModel}, the criterion will usually be the dependent variable, and the cues the predictor variables. For a \code{ResponseModel}, the dependent variable will usually be the response variable, and the predictor variables (some function of) the predictions of the learning model. Both learning and response model can contain free variables.

\section{Implementation of the GCM}

A GCM can be specified by the function \code{GCM(formula,data,parameters,...)}. This function creates an object of the class \code{GCM}, extending the \code{LearningModel} class. To estimate the model's parameters, we can call the \code{estimate(object,...)} method. This will result in estimates which maximize the likelihood of the criterion. However, we will often want to use the \code{GCM} model as part of an \code{McplModel}. The ``classical'' GCM \cite{Nosofsky86} uses
\begin{equation}
P(R=j) = \frac{S_j^\lambda}{\sum_{k=1}^K S_k^\lambda}
\end{equation}
which is implemented in \code{mcplR} by a \code{RatioRuleResponse} model. This can be created by calling \code{RatioRuleResponse(formula,...)}. 


, where \code{mod} is the model specified by the \code{GCM} function. Starting values for the parameters can be given by the \code{parameters} argument. If this argument is not given, default starting values will be used.

\subsection{Learning Model}

The GCM 

\subsection{Response Model}

The GCM assumes that responses follow Luce's ratio rule, such that
\begin{equation}
P(R = j|\vec{x}) = \frac{V_j^\lambda}{\sum_k V_k^\lambda}
\end{equation}
Note that that the value of $V_j$ is given by the learning model. The response model thus contains a single free parameter, $\lambda$.

\subsection{Estimation}

The freely estimable parameters of the GCM are collected in the parameter vector $\greekv{\theta} = (,\lambda)$. Maximum likelihood estimates of these parameters are obtained by numerical maximisation of the likelihood $P(R|\greekv{\theta})$, specified in the response model. By default, numerical optimization is done by the Nelder-Mead simplex algorithm as implemented in the \code{optim} function of \code{R}. Other estimation procedures can be specified at the level of specific classes deriving from \code{McplModel}. There is also a switch to allow conditional maximisation of the response model parameters. In some cases, maximum likelihood estimates of the response model parameters can be derived in closed form (conditional on the learning model parameters), so that conditional estimation can decrease the computation required.


\end{document}
