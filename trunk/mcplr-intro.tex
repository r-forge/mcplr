\documentclass[doc]{apa}
\usepackage[utopia]{mathdesign}
\usepackage{graphicx,apacite,amsmath,rotating,verbatim,epsfig,subfigure}
\usepackage{dcolumn}
\usepackage{tikz}

\title{mcplR: An R package to fit categorization and other multiple cue probability learning models}
\author{Maarten Speekenbrink}
\affiliation{Department of Cognitive, Perceptual and Brain Sciences \\ University College London}

\abstract{
This paper introduces the mcplR package, an open source package for the R software environment to fit various multiple cue learning models. The package is introduced through the implementation of the Rescorla-Wagner model. A brief description of the other models currently implemented, such as the GCM and ALCOVE. 
}

\ifapamodedoc{%
\leftheader{Speekenbrink}
\rightheader{mcplR}
\acknowledgements{This research was supported by the ESRC Centre for Economic Learning and Social Evolution (ELSE).

Address correspondence to M. Speekenbrink, Department of Psychology, University College London, Gower Street, London WC1E 6BT, England, e-mail: \texttt{m.speekenbrink@ucl.ac.uk}}
}

\ifapamodejou{%
\leftheader{Speekenbrink}
\rightheader{mcplR}
\acknowledgements{Address correspondence to M. Speekenbrink, Department of Psychology, University College London, Gower Street, London WC1E 6BT, England, e-mail: \texttt{m.speekenbrink@ucl.ac.uk}}
}

\ifapamodeman{%
	\note{
	\vspace{6em}
	\begin{flushleft}
    Dr. M. Speekenbrink\\
    Department of Psychology\\
    University College London \\
    Gower Street \\
    London WC1E 6BT \\ 
    England\\
    Tel:  +44 (0) 20 7679 7572 \\
    Fax: +44 (0) 20 7436 4276 \\
    E-mail: m.speekenbrink@ucl.ac.uk\\
   \end{flushleft}}}
{% else, i.e., in jou and doc mode 
\note{Draft of \today}}

\journal{To be submitted}

\renewcommand{\vec}[1]{\text{\bf{#1}}}
\newcommand{\mat}[1]{\text{\bf{#1}}}
\newcommand{\tr}{\text{tr}}
\newcommand{\logit}[1]{\log \left( \frac{#1}{1 - #1} \right)}
\newcommand{\logodds}[2]{\log \left( \frac{#1}{#2} \right)}
\newcommand{\slogit}[1]{\log( \tfrac{#1}{1 - #1})}
\newcommand{\mean}[1]{\overline{#1}}
\newcommand{\sign}{\text{sgn}}
\newcommand{\tif}{\text{ if }}

\newcommand{\greekv}[1]{\mbox{\boldmath$#1$}}
\newcommand{\greekm}[1]{\mbox{\boldmath$#1$}}
\newcommand{\thetab}{\mbox{\boldmath$\theta$}}
\newcommand{\betab}{\mbox{\boldmath$\beta$}}
\newcommand{\mub}{\mbox{\boldmath$\mu$}}
\newcommand{\Sigmab}{\mbox{\boldmath$\Sigma$}}
\newcommand{\sigmab}{\mbox{\boldmath$\sigma$}}
\newcommand{\MSE}{\text{MS}_e}
\newcommand{\diag}{\text{diag}}
\newcommand{\mtr}{^{\textsf{T}}}

\newcommand{\code}[1]{{\ttfamily{#1}}}
%\newcommand{\code}[1]{{\verb{#1}}}

\newcolumntype{d}[1]{D{.}{.}{#1}}

\begin{document}
\maketitle

Multiple Cue Probability Learning (MCPL) tasks involve predicting a criterion variable on the basis of a number of cues which are probabilistically related to the criterion. This is a general type of task, and a number of paradigms in (cognitive) psychology fall under its header. When the criterion is a categorical variable, they are usually referred to as (probabilistic) category learning tasks. When the criterion is a metric variable, they are often referred to as function learning or Social Judgement tasks. A large number of formal models have been proposed to describe how people learn to perform MCPL tasks. We describe here an \code{R} package in which a number of these have been implemented, and which has been designed such that users can relatively easily add other models. 

We will introduce the basic features of the package through its implementation of the Rescorla-Wagner model. We will also briefly illustrate how to specify a new model. 

\section{The general structure of MCPL models}

In MCPL tasks, the objective is to predict a criterion variable $Y_t$ on the basis of a number of cues $X_{j,t}$. On each trial $t = 1,\ldots,T$, cue values are presented, and a prediction $R_t$ is made about the value of the criterion.

\begin{figure}
\centering
\begin{tikzpicture}[scale=.9]
		\begin{scope}[]{
			\pgfsetxvec{\pgfpoint{2.4cm}{0cm}}
			\pgfsetyvec{\pgfpoint{0cm}{1.4cm}}
			\tikzstyle{every node}=[minimum size=.8cm]


				\draw	node[draw,circle] (thetaR1) at (0,-2) {$\greekv{\theta}_R$};
				\draw node[draw,circle] (thetaR2) at (1,-2) {$\greekv{\theta}_R$};
			  \draw	node[draw,circle] (thetaR3) at (2,-2) {$\greekv{\theta}_R$};
				\draw	node (thetaRdots) at (3,-2) {$\ldots$};
				\draw	node[draw,circle] (thetaRT) at (4,-2)  {$\greekv{\theta}_R$};
				
				\draw	node[draw] (r1) at (0,-1) {$R_1$};
				\draw node[draw] (r2) at (1,-1) {$R_2$};
			  \draw	node[draw] (r3) at (2,-1) {$R_3$};
				\draw	node (rdots) at (3,-1) {$\ldots$};
				\draw	node[draw] (rT) at (4,-1)  {$R_T$};

				\draw	node[draw,circle] (eta1) at (0,0) {$\greekv{\eta}_1$};
				\draw node[draw,circle] (eta2) at (1,0) {$\greekv{\eta}_2$};
			  \draw	node[draw,circle] (eta3) at (2,0) {$\greekv{\eta}_3$};
				\draw	node (etadots) at (3,0) {$\ldots$};
				\draw	node[draw,circle] (etaT) at (4,0) {$\greekv{\eta}_T$};
								
				\draw node[circle,draw] (psi1) at (0,1) {${\greekv{\psi}}_1$};
				\draw node[circle,draw] (psi2) at (1,1) {${\greekv{\psi}}_2$};
				\draw	node[circle,draw] (psi3) at (2,1) {${\greekv{\psi}}_3$};
				\draw	node[circle] (psidots) at (3,1) {$\ldots$};
				\draw	node[circle,draw] (psiT) at (4,1) {${\greekv{\psi}}_T$};
							
				\draw node[draw] (x1) at (0,2) {$\vec{x}_1$};
				\draw node[draw] (x2) at (1,2) {$\vec{x}_2$};
				\draw	node[draw] (x3) at (2,2) {$\vec{x}_3$};
				\draw	node (xdots) at (3,2) {$\ldots$};
				\draw	node[draw] (xT) at (4,2) {$\vec{x}_T$};
				
				\draw node[draw] (y1) at (0,3) {$y_1$};
				\draw	node[draw] (y2) at (1,3) {$y_2$};
				\draw	node[draw] (y3) at (2,3) {$y_3$};
				\draw	node (ydots) at (3,3) {$\ldots$}; 
				\draw	node[draw] (yT) at (4,3) {$y_T$};

				\draw	node[draw,circle] (thetaL1) at (0,4) {$\greekv{\theta}_L$};
				\draw node[draw,circle] (thetaL2) at (1,4) {$\greekv{\theta}_L$};
			  \draw	node[draw,circle] (thetaL3) at (2,4) {$\greekv{\theta}_L$};
				\draw	node (thetaLdots) at (3,4) {$\ldots$};
				\draw	node[draw,circle] (thetaLT) at (4,4)  {$\greekv{\theta}_L$};
				
				\draw[->] (psi1) -- (eta1);
				\draw[->] (psi2) -- (eta2);
				\draw[->] (psi3) -- (eta3);
				\draw[->] (psiT) -- (etaT);
				
				\draw[->] (x1) .. controls (-.5,1.5) and (-.5,0.5) .. (eta1);
				\draw[->] (x2) .. controls (.5,1.5) and (.5,0.5) ..  (eta2);
				\draw[->] (x3) .. controls (1.5,1.5) and (1.5,0.5)  ..  (eta3);
				\draw[->] (xT) .. controls (3.5,1.5) and (3.5,0.5) ..  (etaT);
				
				\draw[->] (eta1) -- (r1);
				\draw[->] (eta2) -- (r2);
				\draw[->] (eta3) -- (r3);
				\draw[->] (etaT) -- (rT);				
				
				
				\draw[->] (thetaR1) -- (r1);
				\draw[->] (thetaR2) -- (r2);
				\draw[->] (thetaR3) -- (r3);
				\draw[->] (thetaRT) -- (rT);
				
				\draw[->] (psi1) -- (psi2);
				\draw[->] (psi2) -- (psi3);
				\draw[->] (psi3) -- (psidots);
				\draw[->] (psidots) -- (psiT);		
						
				\draw[->] (x1) -- (psi2);
				\draw[->] (x2) -- (psi3);
				\draw[->] (x3) -- (psidots);
				\draw[->] (xdots) -- (psiT);
							
				\draw[->] (y1) -- (psi2);
				\draw[->] (y2) -- (psi3);
				\draw[->] (y3) -- (psidots);
				\draw[->] (ydots) -- (psiT);
				
				\draw[->] (thetaL1) -- (psi2);
				\draw[->] (thetaL2) -- (psi3);
				\draw[->] (thetaL3) -- (psidots);
				\draw[->] (thetaLdots) -- (psiT);
				}
			\end{scope}

\end{tikzpicture}
\caption{Representation of an MCPL model. The learning model involves updating representations $\greekv{\psi}_{t+1}$ on the basis of $\vec{x}_t$ and $y_t$. Representations and cues are used to form a prediction $\greekv{\eta}_t$, which is the basis of responses $R_t$.}
\label{fig:mcplmodel}
\end{figure}

An MCPL model consists of a learning and response models. A learning model describes how a representation of the environment is updated as new data comes in. A response model describes how this representation is used to form predictions of the environment. A graphical representation is given in Figure~\ref{fig:mcplmodel}. A main feature of MCPL models is that responses $R_t$ are conditionally independent given predictions $\eta_t$, i.e.
\begin{equation}
P(R_1,\ldots,R_T|\greekv{\eta}_1,\ldots,\greekv{\eta}_T,\greekv{\theta}_R) = \prod_{t=1}^T P(R_t|\greekv{\eta}_t,\greekv{\theta}_R)
\end{equation}
The predictions follow from the task representations $\psi_t$ and probe cue patterns $\vec{x}_t$, e.g.
\begin{equation}
\eta_t = f(\psi_t,\vec{x}_t).
\end{equation}

While the response model is always a statistical model, in many cases the learning model is not. In many learning models, representations are updated as
\begin{equation}
\greekv{\psi}_{t+1} = g(\greekv{\psi}_t,\vec{x}_t,y_t,\greekv{\theta}_L),
\end{equation}
where $g$ is some (non-linear) function. For instance, the Rescorla-Wagner model \cite{Rescorla72} involves updating stimulus response associations $\greekv{\psi}_t$ in this form. As we will see later, the GCM is different, but still deterministic, in the sense that a particular set of paired cue and criterion values $(\vec{x}_{1:t},y_{1:t})$ and parameters $\greekv{\theta}_L$ always result in the same representation $\greekv{\psi}_t$. There are exceptions to this (such as Anderson's \citeyear{Anderson91} Rational Model of Categorization), but these are currently not implemented in \code{mcplR}. Note that determinism is not a requirement to implement learning models in \code{mcplR}. However, the default estimation relies numerical optimization routines which will give inconsistent results if $\greekv{\psi}_{1:T}$ differs from iteration to iteration. Hence, for probabilistic learning models, a different estimation routine will have to be implemented.
 
\section{Implementation in \code{mcplR}}

MCPL models are implemented in \code{mcplR} through the \code{McplModel} class. The class has two slots: one for a \code{LearningModel} and one for a \code{ResponseModel}. Both these derive from the same basic class and consist minimally of a (predictor) matrix $x$, a (criterion) matrix $y$, and a parameter list. For the learning model, the predictor matrix will usually contain the cues \vec{x}, and the criterion matrix the criterion $Y$. For the response model, the predictor matrix will usually contain $\greekv{\eta}$, and the response matrix the responses $R$. 

%At the heart of the \coded{mcplR} package is the class \code{McplModel}. An \code{McplModel} contains of two submodels, namely a \code{LearningModel} and a \code{ResponseModel}. Both these are derived from a general \code{McplBaseModel}, which contains a slot for a dependent variable ($y$), a slot for predictor variables ($x$), and a slot for a list with parameters. For a \code{LearningModel}, the criterion will usually be the dependent variable, and the cues the predictor variables. For a \code{ResponseModel}, the dependent variable will usually be the response variable, and the predictor variables (some function of) the predictions of the learning model. Both learning and response model can contain free variables.

In addition to class definitions, the \code{mcplR} package defines a number of methods which operate on these classes. The most important from the users viewpoint is no doubt the \code{estimate} method, by which to obtain (ML) estimates of model parameters. By default, estimate uses the \code{optim} function of R to numerically minimize the negative log-likelihood of the model. If the \code{logLik} method is unavailable, the least squares estimation will be attempted.

In the following section, we will illustrate the various options with an example 

\section{Example: the Rescorla-Wagner model}

The Rescorla-Wagner model \cite{Rescorla72} is a well-known associative learning model. It describes the strengthening of associative links between unconditioned stimuli $Y$ and conditioned stimuli $X$. On a given trial, a number of conditioned stimuli are presented and associated with an unconditioned stimulus. From these paired presentations, a (human) animal learns to expect the unconditioned stimulus when encountering the conditioned stimulus, and as a result shows a conditioned response. The expectation is due to the formation of associative links between conditioned and unconditioned stimulus, resulting in a link between the conditioned stimulus and the conditioned response. 

From trial $t$ to $t+1$, the associative strength between a CS (cue) $j$ and US (criterion) $k$ is updated as
\begin{equation}
V_{j,k}(t+1) = \begin{cases} \psi_{j,k}(t) + \alpha_j \beta_{k,2} [\lambda_k - \sum_j x_j(t) V_{i,k}(t)] \quad \text{when US $k$ is present at trial $t$} \\
V_{j,k}(t) + \alpha_j \beta_{k,2} [0 - \sum_j x_j(t) V_{i,k}(t)] \quad \text{when US $k$ is absent on trial $t$} \\
\end{cases}
\end{equation}
where $\lambda_k$ is the maximum level of associative strength of US $k$ at trial $t$ (this should be 0 when US $k$ is absent on trial $t$), $x_j(t)$ the value of the CS $j$ at trial $t$ (0 when CS $j$ is not present at $t$), $\alpha_j$ the salience of CS $j$, and $\beta_k$ the salience of the US $k$, which can be dependent on the presence or absence of $y_k(t)$.

Let $\vec{y}_t = (y_ty_k(t)$ be a variable with value 0 when US $k$ is absent on trial $t$, and $\lambda$ when US $k$ is present (note that this does not have to be a single value, e.g., US $k$ can be present in different magnitudes). Similarly, let  
As responses will normally depend on the expectancy, we let $\eta_k(t) = \sum_j V_{j,k} x_j$

\subsection{Creating a Rescorla-Wagner learning model}

A Rescorla-Wagner model can be created by calling \code{RescorlaWagner(formula,data,...)}. For instance, for an event $y$ with two levels, and four cues $x_j$, we can use 

\code{lMod <- RescorlaWagner(y \textasciitilde x1+x2+x3+x4,data=WPT,ntimes=c(200,200),remove.intercept=TRUE)}

which will create a Rescorla-Wagner model with default parameters. We can view the default parameter values by calling \code{getPars(lMod)}, which shows the contents of the model's parameter list. By default, the model has a single $\alpha$ parameter, identical for each cue, initialized at $\alpha = 0.1$ All $\beta$ parameters are initialized to 1, while the associative strength of each cue is initialized at 0. The \code{ntimes} argument is used to indicate that the dataset contains data of two participants, each consisting of 200 learning trials. The \code{remove.intercept} argument is used to force removal of the intercept that R includes in the design matrix by default. 

\subsection{Creating a response model}

A common choice is the (exponentiated) ratio response rule:
\begin{equation}
P(R=j) = \frac{\exp ( \beta \psi_j)} { \sum_k \exp (\beta \psi_k) }
\end{equation}
which we can create by calling
\code{rMod <- RatioRuleResponse(r \textasciitilde 1,data=WPT)}

\subsection{Estimating the model parameters}

After creating a learning and response model, we can combine them into an \code{McplModel}. We can then estimate the learning and response model parameters simultaneously.

\begin{figure}
\begin{verbatim}
# create a learning model
lMod <- RescorlaWagner(y ~ x1+x2+x3+x4,data=WPT,ntimes=c(200,200),remove.intercept=TRUE)
# create a response model
rMod <- RatioRuleResponse(r~1,data=WPT,ntimes=c(200,200))
# combine in an McplModel
mMod <- McplModel(lMod,rMod)
# estimate the free parameters
mMod <- fit(mMod)
# summary
summary(mMod)
\end{verbatim}
\caption{Example code to create and estimate a Rescorla Wagner model with a ratio response rule.} 
\end{figure}

\section{Other learning models currently implemented in \code{mcplR}}

\subsection{Single Layer Feedforward Network (SLFN)}

Associative learning models such as the Rescorla-Wagner model can be implemented by an artificial neural network \cite{Gluck88} with one input layer and one output layer. These networks are also known as Single Layer Feedforward Network models \cite<SLFN,>{Bishop95}. The SLFN model offers a general implementation which learns the connection weights by the delta (or LMS) rule. 

\begin{equation}
\vec{w}_{t+1}^{(i,j)} = \vec{w}_{t}^{(i,j)} + \frac{\greekv{\eta}^{(i,j)}}{t^{\alpha^{(i,j)}}} \delta^{(i,j)}_t + \greekv{\beta}^{(i,j)}(\vec{w}_t^{(i,j)} - \vec{w}^{(i,j)}_{t-1})
\end{equation}
where $\delta^{(i,j)}_t$ is the gradient of the activation function, e.g.
\begin{equation}
\delta^{(i,j)}_t = (Y_{t}^{(j)} - \vec{w}_t^{(i,j)} \vec{x}_t^{(i)}) \vec{x}_t^{(i)}
\end{equation}
if the activation function is linear. The parameters $\alpha$, $\beta$, and $\eta$ are usually scalars, so that they are identical for all cues $i$ and criterion values $j$. But they can optionally be vectors or matrices. 

A SLFN learning model can be created by the function \code{SLFN(formula,data,...)}. 

\subsection{Generalized Context Model (GCM)}

The Generalized Context Model \cite<GCM,>{Nosofsky86}. 

GCM learning model can be created by the function \code{GCM(formula,data,...)}. This will create a default GCM model, with a city-block distance function and an exponential similarity gradient, with  The distance function can be changed to an euclidian or general Minkowski distance function by the option \code{distance=``euclidian''} or \code{distance=``minkowksi''}, respectively. An arbitrary function can also be used by using a function for the distance option. Similarly, the similarity gradient can be changed through the option \code{similarity=``gaussian''}. In addition, the version of the GCM allows the specification of a sampling function (or memory gradient). By default, this is set to uniform, but other options are a power and exponential function.
 
\subsection{ALCOVE}

ALCOVE \cite{Kruschke92}.

\section{Extending \code{mcplR}}

The \code{mcplR} package was designed with the objective of being relatively easily extendable. 

To add a new learning model, users must define a new class which inherits from the \code{LearningModel} class. Then, they should minimally define a \code{predict} method. Ideally, other methods, such as \code{logLik}, would also be defined, but these are not required to estimate the models parameters when incorporated into an \code{McplModel}. In many cases, users can rely on the default methods.

To add a response model, users must again define a new class which inherits from the \code{ResponseModel} class. In addition, they must also minimally define either the \code{logLik} method or the \code{predict} method. If only the latter is defined, the model parameters will be estimated by OLS, while if the former is defined, ML estimates can be obtained. 

\section{Useful methods and functions}

In addition to the \code{estimate} method, there are default methods for model evaluation. In particular, standard model selection criteria, such as the AIC, BIC, and (pseudo-) $R^2$ are defined. In addition, many models have default plotting methods. For instance, calling \code{plot(tMod@learningModel)}, where \code{tMod} is the one estimated in the example, we obtain the plot of Figure XXX. 

\begin{table}
\caption{Selected methods and functions in the mcplR package}
\begin{tabular}{lll} \hline
name & description & example call \\ \hline
\code{plot} & default plots of mcpl models & \code{plot(mod)} \\
\code{simulate} & simulate responses from a \code{ResponseModel} (or the response part of an \code{McplModel}) & \code{simulate(mod)} \\
\hline
\end{tabular}
\end{table}

\section{Conclusion}

We introduced the \code{mcplR} extension package for the R language and framework. The package provides a flexible framework to specify and estimate learning models. A number of common models is included. 

In the future versions of \code{mcplR}, we plan to:
\begin{itemize}
\item incorporate other learning and response models
\item allow more general parameter constraints
\end{itemize}

\bibliography{bibliography}



\end{document}

A GCM can be specified by the function \code{GCM(formula,data,parameters,...)}. This function creates an object of the class \code{GCM}, extending the \code{LearningModel} class. To estimate the model's parameters, we can call the \code{estimate(object,...)} method. This will result in estimates which maximize the likelihood of the criterion. However, we will often want to use the \code{GCM} model as part of an \code{McplModel}. The ``classical'' GCM \cite{Nosofsky86} uses
\begin{equation}
P(R=j) = \frac{S_j^\lambda}{\sum_{k=1}^K S_k^\lambda}
\end{equation}
which is implemented in \code{mcplR} by a \code{RatioRuleResponse} model. This can be created by calling \code{RatioRuleResponse(formula,...)}, where only the left-hand side of the \code{formula} is evaluated. Starting values for the parameters can be given by an optional \code{parameters} argument. If this argument is not given, default starting values will be used (in this case, $\lambda=1$).

\subsection{Learning Model}

The GCM is different from other learning models in that it does not involve . Rather, learning consists of storing exemplars $(\vec{x}_t,y_t)$. To keep within the present framework, we can take the representation $\greekv{\psi}_t(\vec{x})$ to be a function 

\subsection{Response Model}

The GCM assumes that responses follow Luce's ratio rule, such that
\begin{equation}
P(R = j|\vec{x}) = \frac{V_j^\lambda}{\sum_k V_k^\lambda}
\end{equation}
Note that that the value of $V_j$ is given by the learning model. The response model thus contains a single free parameter, $\lambda$.

\subsection{Estimation}

The freely estimable parameters of the GCM are collected in the parameter vector $\greekv{\theta} = (,\lambda)$. Maximum likelihood estimates of these parameters are obtained by numerical maximisation of the likelihood $P(R|\greekv{\theta})$, specified in the response model. By default, numerical optimization is done by the Nelder-Mead simplex algorithm as implemented in the \code{optim} function of \code{R}. Other estimation procedures can be specified at the level of specific classes deriving from \code{McplModel}. There is also a switch to allow conditional maximisation of the response model parameters. In some cases, maximum likelihood estimates of the response model parameters can be derived in closed form (conditional on the learning model parameters), so that conditional estimation can decrease the computation required.
